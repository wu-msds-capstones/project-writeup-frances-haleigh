# Methods
## Process outlines:
::: {.panel-tabset}
### Goal 1
**Predicting disease prevalence by insurance type**

1. Manipulate data to find mean prevalence of the disease by state. Use this state-wide data rather than the county data as the Medicaid covariates were only available by state. 
2. Preprocess, split, and model the data as fully outlined in Goal 3.

### Goal 2
**Detecting differences in access to care by insruance type**

Need to fill out. Use R or Python?

### Goal 3
**Predicting health by access to care**

*Statistics in R*

1. Create our desired dataframe by reading in data from our remote database and joining where necessary.
2. Preprocess:
    i) Selected preventative health measures of interest.
    ii) Create distribution plots for variables of interest. Take natural log if skew was obvious (income data).
3. Create Model
    i) Multiple Linear Mixed Model in R using selected variables. Created lm models for life expectancy, infant mortality, and age-adjusted death rate. 
    ii) select signfiicant features for each, and recreate model with signfiicant features.
    iii) check to see if significance changes after recreating the model.
    iv) check directions of significance and double check variables have enough data points.
4. Kaplan-Meier Curves for Life Expectancy
    i) Break up each significant predictive variable from above into quartiles.
    ii) Create KM curves.
    iii) Run log rank test to determine if there were significant differences between the quartiles.

*Machine Learning in Python*

1. Create our desired dataframe by reading in data from our remote database and joining where necessary.
2. Preprocess:
    i) Drop NAs while optimizing data frame size via trial and error.
    ii) Use StandardScaler and OneHotEncoder tools from sklearn to transform our numeric and categorical data, respectively.
3. Use KMeans to find the top 10 features from each model. (See <a href="./Figures/kbest_life_exp.png" title="Figure S.1A">Supplemental Figures 1A</a>, <a href="./Figures/kbest_inf_mor.png" title="Figure S.1B">1B</a>, and <a href="./Figures/kbest_mortality.png" title="Figure S.1C">1C</a>). Combine these variables with significant variables from the linear regression models completed in R.
4. Continue preprocessing with the reduced dataset (using only variables from step 3) 
    i) Bin and modify data as necessary. This ensured equal representation across classes for each predicted y variable.
        A. In order to use classification tools such as RandomForestClassifier or GradientBoostingClassifier, we binned all our data by quartiles before running it through our models. 
        B. For life expectancy, we dropped outliers first (ages below 60 and above 90, see <a href="./Figures/life_expectancy_bins.png" title="Figure S.1D">Supplemental Figure 1D</a>).
    ii) Train/test split dataset
5. Create model
    i) Use LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, CategoricalNaiveBayes, and Ensemble. Find which model performs the best.
    ii) Hyperparameterize the model that works best from step 5i.
    iii) Cross validate finalized model
    iv) Test on testing data to find true accuracy
    v) Confusion matrix to see what went wrong (see <a href="./Figures/conf_matrix_life_exp.png" title="Figure S.1E">Supplemental Figures 1E</a> and <a href="./Figures/conf_matrix_mortality.png" title="Figure S.1F">1F</a>).
6. Add PCA to the model
    i) Determine how many components explain 90% variance in the model (see <a href="./Figures/variance_life_exp.png" title="Figure S.1G">Supplemental Figures 1G</a>, <a href="./Figures/variance_inf_mor.png" title="Figure S.1H">1H</a>, and <a href="./Figures/variance_mortality.png" title="Figure S.1I">1I</a>).
    ii) Add PCA with that number of components to the optimized pipeline made from Step 5.

### Goal 4
**Engineering overall health variable**

1. Gather our entire dataset (not just access to care related variables) See [@sec-s_table_1] for a list of all variables used.
2. Preprocess the dataset using StandardScaler and OneHotEncoder to the appropriate columns.
3. Use PCA to see how much variance can be explained by the first eigenvalue (see <a href="./Figures/total_eigenvalue.png" title="Figure S.1J">Supplemental Figure 1J</a>).
4. Project desired health variables to the first eigenvalue.
5. Compare weights of all variables to see how much they each contribute to this “new variable."
:::

## Statistical Thinking
We determined health status using commonly used proxies such as life expectancy and mortality rates.³ We used statistics (log rank tests) and modeling (Kaplan-Meier curves and linear regression) to determine how access to preventative care correlates with health status. We compared proportions across states to account for differences in population.

In our linear models created in R, we first took the natural logarithm of income-related data due to both its skewed distribution and disproportionate scale (in comparison to the rest of our data). 

::: {.callout-note collapse="true"} 
#### What are these statistical tests?
|**Test**                |**Description**                                                                              |
|------------------------|---------------------------------------------------------------------------------------------|
|*Kaplan-Meier Curve*    |A Kaplan-Meier (KM) curve is a display of survival data over time and across groups.         |
|*Log Rank Test*         |A log rank test is used to determine if the groups in a KM curve are significant.            |
|*Linear Regression*     |Linear regression is a statistical test used to find and quanitfy linear trends between data.|
|*Add any test used in goal 2*|                                                                                        |      
:::

## Data Visualization
We used data visualization as part of both our exploratory data analysis and in our final outcome. We created maps of the United States to determine if there is a spatial relationship between our variables and quality of health. Various other visualizations were displayed from our statistical analysis and machine learning product, such as Kaplan-Meier curves, graphs of K-Best's top features, confusion matrices, and K-Means and/or Gaussian Mixture Model scatterplots, where appropriate.  

## Data Engineering
We stored our data into a relational database and accessed it with Postgresql (through Railway). We accessed our data in both R and Python using the RPostgreSQL and sqlalchemy libraries, respectively. A detailed description for how this was done is in the data section above. 

## Machine Learning
We used machine learning models such as ensembles, logistic regression, random forest, naive bayes, PCA, KMeans, Gaussian Mixture Models, and more to explore relationships and predict health outcomes from our data. For example, we predicted metrics like life expectancy based on our access to care variables. We also predicted the prevalence of common chronic conditions, such as Diabetes, within each state based on the percentage of people on Medicaid. Finally, we created a new variable that represented overall health quality. 

::: {.callout-note collapse="true"}
#### What are these models and tools?
### Model Optimization Tools
|**Name**                    |**Explaination**                                                                                                                                      |
|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
|*K-Best Feature Selection*  |K-Best feature selection is a tool used in machine learning used to find which variables in a given model most contribute to its prediction accuracy. |
|*Confusion Matrices*        |Confusion matrices are plots of the true and predicted values of a given model and are used to determine where a model is succeeding and/or failing.  |

### Data Preprocessing Tools
|**Name**                    |**Explaination**                                                                                                                                      |
|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
|*Standard Scaler*           |Standard Scaler is a tool you can use in machine learning to standardize all numeric inputs so the model can correctly assess the impact of each value.|
|*One Hot Encoder*           |Similar to Standard Scaler, One Hot Encoder is a method use to convert character data to numeric in a standardized way so the model can weigh the impact of character input along with numeric.|

### Supervised Classification Models
|**Name**                    |**Explaination** 
|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
|*Random Forests*            |Random Forests randomly distribute the data given to it into multiple "trees" and averages the outcome from each tree to come to its final result.   |
|*Naive Bayes*               |Naive Bayes uses probability to find the most likely outcome.                                                                                        |
|*Logistic Regression*       |Logisitic regression finds the linear regression of the log odds of the predictor variables to predict a binary outcome.                             |
|*Gradient Boosting*         |Rather than creating lots of uncorrelated models like Random Forest, Gradient Boosting fits many models where each successive one corrects upon where the previous one was wrong.|
|*Ensemble*                  |Ensemble models combine many user-specified models and comes to its final outcome by averaging the individual outcomes of each model made up within it.|  

### Unsupervised Dimension Reduction Model
|**Name**                    |**Explaination** 
|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
|*PCA*                       |Principle Component Analysis (PCA) is a method of reducing the number of variables in a model by combining them into the principle components that describe the most variance in a given dataset.|

### Unsupervised Clustering Models
|**Name**                    |**Explaination** 
|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
|*K-Means*                   |K-Means is a clustering method that attempts to divide up the data into a specified number of groups based on how close each data point is to each other.|
|*Gaussian Mixture Model*    |Gaussian Mixture Model is another clustering method that assumes a specificed number of normally distributed clusters and assigns data points to the normal curve they best fit.|
:::

## Data Ethics
We were wary of furthering the gap between classes, races, and/or genders. Often, those in lower socioeconomic status tend to have lower quality of health and less access to affordable healthcare. Our goal was not to perpetuate this gap but to find potential solutions for these underserved populations. We also recognized that our datasets were incomplete and that we were not be able to represent everyone’s needs. Those who are not using well-known forms of insurance, such as Direct Primary Care, or who are not seeking traditional medical care are not represented.  



